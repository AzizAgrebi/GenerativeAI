<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LaTeX Article</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: 'Calibri';
            margin: 0;
            padding: 0;
            background-color: #f5f5f5;
        }

        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 20px 50px;
            color: white;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; 
            background-color: #000;
            width: 94.4%;
        }

        .logo {
            font-size: 24px;
        }

        .navigation {
            display: flex;
        }

        .navigation a {
            margin-left: 20px;
            text-decoration: none;
            color: inherit;
        }

        .navigation a:first-child {
            margin-left: 0;
        }

        .content-area {
            margin: 20px auto;
            max-width: 1000px;
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
        }
        
        .margin {
            width: calc((100% - 800px) / 2);
            background-color: #dcdcdc; /* Elegant gray color for margin */
            height: 100vh;
            position: fixed;
            top: 0;
            left: 0;
        }

        .margin.right {
            left: auto;
            right: 0;
        }

        .math {
            display: inline-block;
            font-size: 16px;
            line-height: 1.5;
        }

    </style>

</head>

<body>

    <header>
        <div class="logo">GenerativeAI</div>
        <nav class="navigation">
            <a href="#">Home</a>
            <a href="#">About</a>
            <a href="#">Links</a>
        </nav>
    </header>

    <div class="content-area">
        <h1 id="introduction">Introduction</h1>
        <p>Diffusion models have emerged as a leading class of generative
        models, capable of generating diverse and high-resolution images. Their
        prominence escalated notably when major entities, such as OpenAI and
        Google, achieved success in training highly performing models like
        DALLE-2 and Imagen. In this article, we will delve into the mathematical
        foundations of diffusion models.</p>
        <h1 id="the-key-idea">The key idea</h1>
        <p>When confronted with observed samples x stemming from a distribution
        of interest — considering these samples as images — the aim of a
        generative model is to effectively capture and model the underlying data
        distribution <span class="math inline">\(q(x)\)</span>. Once this
        modeling is achieved, the model gains the capability to generate new
        samples at will. In the case of diffusion models, the approach to
        estimating the true data distribution <span
        class="math inline">\(q(x)\)</span> involves the following methodical
        process : It begins by taking an initial image <span
        class="math inline">\(x_0\)</span> and gradually adding Gaussian noise
        to it through a series of T steps. The goal is that after T steps, the
        noised image <span class="math inline">\(x_T\)</span> follows a normal
        distribution. This step is called the forward diffusion process.
        Following this, a neural network is trained to reverse the noising
        process and recover the original data. This is the ability to
        proficiently model the reverse process that enables the diffusion model
        to generate new data. Indeed, if we learn to reverse the noising
        process, we can sample a random noise from the normal distribution (the
        distribution of <span class="math inline">\(xT\)</span> and apply the
        reverse of the noising process to generate a new data that follows the
        same distribution as x0. This is the called the reverse diffusion
        process or, in general, the sampling process of the diffusion model. The
        key is to successfully learn the reverse process of our noising
        process.</p>
        <h1 id="the-forward-diffusion-process">The forward diffusion
        process</h1>
        <p>Given a data-point <span class="math inline">\(x_0\)</span> sampled
        from the real data distribution <span
        class="math inline">\(q(x)\)</span> (<span class="math inline">\(x_0
        \sim q(x)\)</span>), we establish the forward diffusion process by
        incorporating Gaussian noise at each step. More precisely, at every step
        <span class="math inline">\(t\)</span> (<span class="math inline">\(1
        \leq t \leq T\)</span>) of this process, we introduce Gaussian noise
        with a variance of <span class="math inline">\(\beta_t\)</span> to <span
        class="math inline">\(x_{t-1}\)</span>, producing a new latent variable
        <span class="math inline">\(x_t\)</span> with distribution <span
        class="math inline">\(q(x_t | x_{t-1})\)</span>. This diffusion process
        is formulated as follows:</p>
        <p><span class="math display">\[\forall t \quad q(x_t | x_{t-1}) =
        \mathcal{N}(x_t; \mu_t = (1 - \beta_t) x_{t-1}, \Sigma_t = \beta_t
        I)\]</span></p>
        <p>Each latent variable <span class="math inline">\(x_t\)</span> only
        depends on the previous variable <span
        class="math inline">\(x_{t-1}\)</span>, making our diffusion process a
        Markov chain.</p>
        <p>As mentioned previously, we desire <span
        class="math inline">\(x_T\)</span> to follow the normal distribution,
        meaning <span class="math inline">\(x_T \sim \mathcal{N}(x_T; \mu_T = 0,
        \Sigma_T = I)\)</span>. We will revisit this when discussing our
        objective function.</p>
        <p>We can conclude this part by calculating the distribution of the
        trajectory of our forward diffusion process conditioned on the given
        data-point <span class="math inline">\(x_0\)</span>. The trajectory is a
        sequence of states that the system traverses as it evolves over time
        (excluding the first state here). Mathematically, the trajectory of our
        process is represented as the sequence <span class="math inline">\((x_1,
        \ldots, x_{T-1}, x_T)\)</span> and is denoted <span
        class="math inline">\(x_{1:T}\)</span>. Therefore, we can write:</p>
        <p><span class="math display">\[\begin{aligned}
            q(x_{1:T} | x_0) &amp;= \frac{q(x_{1:T}, x_0)}{q(x_0)} \quad
        (\text{Bayes&#39; rule}) \\
            &amp;= \frac{q(x_0)\prod_{t=1}^{T} q(x_t | x_{t-1})}{q(x_0)} \quad
        (\text{Markov property}) \\
            &amp;= \prod_{t=1}^{T} q(x_t | x_{t-1})
        \end{aligned}\]</span></p>
        <h1 id="the-reverse-diffusion-process">The reverse diffusion
        process</h1>
        <p>The reverse diffusion process is the process that enables to start
        from <span class="math inline">\(x_T\)</span> (which we assume for now
        to follow a normal distribution) and to finish at a new <span
        class="math inline">\(x_0\)</span> that we will denote <span
        class="math inline">\(\tilde{x_0}\)</span>, which means a new sample
        that follows the distribution <span class="math inline">\(q(x_0) :=
        q(x)\)</span>. Therefore we fix <span class="math inline">\(p(x_T) =
        \mathcal{N}(x_T; \mu_T = 0, \Sigma_T = I)\)</span> and we want to learn
        <span class="math inline">\(q(x_{t-1} | x_t)\)</span> for every t. In
        the general case, each <span class="math inline">\(q(x_{t-1} |
        x_t)\)</span> has no explicit analytical form so we have to train a
        model to approximate this distribution. We will denote <span
        class="math inline">\(p_{\theta}(x_{t-1} | x_t)\)</span> the
        approximation learnt by our model. The model we will use is a U-Net
        model, its parameters are denoted theta here, and we will train this
        model by maximizing the log likelihood. We will detail all of this in
        the next part. One remark is that it was proven that, if the noise added
        at each step during the forward diffusion process is small enough (so if
        <span class="math inline">\(\beta_t\)</span> is small enough), each
        <span class="math inline">\(q(x_{t-1} | x_t)\)</span> is a Gaussian
        distribution. Therefore in this approximation we can write <span
        class="math inline">\(q(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_t,
        \Sigma_t)\)</span> and train our model to only learn the mean and the
        variance of this family of distributions by maximizing the log
        likelihood. In this article, we choose another method. We will show that
        even if <span class="math inline">\(q(x_{t-1} | x_t)\)</span> has no
        explicit analytical form, we can find an explicit analytical form for
        <span class="math inline">\(q(x_{t-1} | x_t, x_0)\)</span> and we will
        try to learn a similar form for <span class="math inline">\(q(x_{t-1} |
        x_t)\)</span> with our model. Everything will make sense when we will
        define our objective function to train our model. Finally, we can do
        like in the previous part and write the distribution of the trajectory
        or our learn reverse diffusion process :</p>
        <h1 id="training-a-diffusion-model">Training a diffusion model</h1>
        <p>In order to learn <span class="math inline">\(p_{\theta}(x_{t-1} |
        x_t)\)</span> with our model to match closely <span
        class="math inline">\(q(x_{t-1} | x_t)\)</span>, we will maximize the
        log-likelihood <span class="math inline">\(ln(q(x))\)</span>. To do so,
        let’s introduce the Evidence Lower Bound (ELBO). For a given
        distribution phi, the general definition of the ELBO is :</p>
        <p>It is possible to show a very interesting relation between the log
        likelihood and the ELBO :</p>
        <p>Interpretation (Log likeli hood = ELBO + DKL)</p>
        <p>This is why for now, we don’t explicitly want to maximize the log
        likelihood but rather the ELBO (which is equivalent given our previous
        result). With a calculus, we can exhibit a very interesting form for the
        ELBO in the case of our diffusion model :</p>
        <p><span class="math display">\[\begin{aligned}
            \ln p(x) &amp;= \ln \int p(x_{0:T}) \, dx_{1:T}\\
            &amp;= \ln \int \frac{p(x_{0:T}) q(x_{1:T}|x_{0})}{q(x_{1:T}|x_{0})}
        \, dx_{1:T} \\
            &amp;= \ln\left(\mathbb{E}_{q(x_{1:T}|x_{0})} \left[
        \frac{p(x_{0:T})}{q(x_{1:T}|x_{0})} \right]\right) \\
            &amp;\geq \mathbb{E}_{q(x_{1:T}|x_0)}
        \left[\ln\left(\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}\right)\right] \\
            &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
        \prod_{t=1}^{T} p_{\theta}(x_{t-1}|x_t)}{\prod_{t=1}^{T}
        q(x_t|x_{t-1})}\right)\right] \\
            &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
        p_{\theta}(x_0|x_1) \prod_{t=2}^{T} p_{\theta}(x_{t-1}|x_t)}{q(x_1|x_0)
        \prod_{t=2}^{T} q(x_t|x_{t-1})}\right)\right] \\
            &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
        p_{\theta}(x_0|x_1)}{q(x_1|x_0)}\right) + \ln\left( \prod_{t=2}^{T}
        \frac{p_{\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1})}\right) \right]\\
            &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
        p_{\theta}(x_0|x_1)}{q(x_1|x_0)}\right) + \ln\left( \prod_{t=2}^{T}
        \frac{p_{\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1}, x_0)}\right) \right]\\
            &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
        p_{\theta}(x_0|x_1)}{q(x_1|x_0)}\right) + \ln\left( \prod_{t=2}^{T}
        \frac{p_{\theta}(x_{t-1}|x_t)}{\frac{q(x_{t-1}|x_t,
        x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}}\right) \right] \\
            &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
        p_{\theta}(x_0|x_1)}{q(x_1|x_0)}\right) +
        \ln\left(\frac{q(x_1|x_0))}{q(x_T|x_0)}\right) + \ln\left(
        \prod_{t=2}^{T} \frac{p_{\theta}(x_{t-1}|x_t)}{q(x_{t-1}|x_t,
        x_0)}\right) \right] \\
            &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
        p_{\theta}(x_0|x_1)}{q(x_T|x_0)}\right) + \sum_{t=2}^{T}
        \ln\left(  \frac{p_{\theta}(x_{t-1}|x_t)}{q(x_{t-1}|x_t, x_0)}\right)
        \right] \\
            &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln
        p_{\theta}(x_0|x_1)\right] + \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln
        \frac{p(x_T)}{q(x_T|x_0)} \right] + \sum_{t=2}^{T}
        \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln
        \frac{p_{\theta}(x_{t-1}|x_t)}{q(x_{t-1}|x_t, x_0)} \right] \\
            &amp;= \mathbb{E}_{q(x_1|x_0)} \left[\ln p_{\theta}(x_0|x_1) \right]
        + \mathbb{E}_{q(x_T|x_0)} \left[\ln \frac{p(x_T)}{q(x_T|x_0)} \right] +
        \sum_{t=2}^{T} \mathbb{E}_{q(x_t, x_{t-1}|x_0)}
        \left[\ln   \frac{p_{\theta}(x_{t-1}|x_t)}{q(x_{t-1}|x_t, x_0)} \right]
        \\
            &amp;= \underbrace{{\mathbb{E}_{q(x_1|x_0)} \left[\ln
        p_{\theta}(x_0|x_1) \right]}}_{\text{Reconstruction term}} -
        \underbrace{D_{KL}\left(q(x_T | x_0) \| p(x_T)\right)}_{\text{Prior
        matching term}} - \underbrace{\sum_{t=2}^{T} \mathbb{E}_{q(x_t|x_0)}
        \left[D_{KL}\left(q(x_{t-1} | x_t, x_0) \|
        p_{\theta}(x_{t-1}|x_t)\right)\right]}_{\text{Denoising matching term}}
        \end{aligned}\]</span></p>
        <p>Calculus of the ELBO.</p>
        <p>Interpretation of the 3 terms.</p>
        <p>Therefore, maximizing the ELBO is pratically equivalent to minimizing
        the third term of our previous written ELBO. ptheta(xt-1|xt) is what we
        want to learn in order to minimize this sum. To be able to do such a
        thing in practice, we need to make sure that we can calculate the other
        terms of the sum, which means q(xt|x0) and q(xt-1|xt, x0) for every t.
        Fortunately, this is the case.</p>
        <p>(1) calculus of <span class="math inline">\(q(x_t|x0)\)</span></p>
        <p><span class="math display">\[\begin{aligned}
            \boldsymbol{x}_t &=\sqrt{\alpha_t} \boldsymbol{x}_{t-1}+\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1} \\
            &= \sqrt{\alpha_t}\left(\sqrt{\alpha_{t-1}} \boldsymbol{x}_{t-2}+\sqrt{1-\alpha_{t-1}} \boldsymbol{\epsilon}_{t-2}\right)+\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1} \\
            & =\sqrt{\alpha_t \alpha_{t-1}} \boldsymbol{x}_{t-2}+\sqrt{\alpha_t-\alpha_t \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2}+\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1} \\
        \\
        &\text But : \\
        \\
            & \sqrt{\alpha_t-\alpha_t \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2}+\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1} \\
            & \sim \sqrt{\alpha_t-\alpha_t \alpha_{t-1}} \mathcal{N}\left(0, \mathbf{I}\right) + \sqrt{1-\alpha_t} \mathcal{N}\left(0, \mathbf{I}\right) \\
            & \sim \mathcal{N}\left(0, (\alpha_t-\alpha_t \alpha_{t-1}) \mathbf{I}\right) + \mathcal{N}\left(0, (1-\alpha_t) \mathbf{I}\right) \\
            & \sim \mathcal{N}\left(0, (\alpha_t-\alpha_t \alpha_{t-1} + 1-\alpha_t) \mathbf{I}\right)  \\
            & \sim \mathcal{N}\left(0, (1-\alpha_t \alpha_{t-1}) \mathbf{I}\right)  \\
            & \sim \sqrt{1-\alpha_t \alpha_{t-1}} \mathcal{N}\left(0, \mathbf{I}\right)  \\
        \\
        &\text Therefore : \\
        \\
            & \boldsymbol{x}_t =\sqrt{\alpha_t \alpha_{t-1}} \boldsymbol{x}_{t-2}+\sqrt{1-\alpha_t \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2} \\
            & =\ldots \\
            & =\sqrt{\prod_{i=1}^t \alpha_i} \boldsymbol{x}_0+\sqrt{1-\prod_{i=1}^t \alpha_i} \boldsymbol{\epsilon}_0 \\
            & =\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_0 \\
            & \sim \mathcal{N}\left(\boldsymbol{x}_t ; \sqrt{\bar{\alpha}_t} \boldsymbol{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)
        \end{aligned}\]</span></p>
        <p>Where <span class="math inline">\(\bar{\alpha}_t := \prod_{i=1}^t
        \alpha_i\)</span></p>
        <p>(2) calculus of q(xt-1|xt, x0)</p>
        <p><span class="math display">\[\begin{aligned}
        & q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)=\frac{q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_0\right) q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_0\right)}{q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right)} \\
        & =\frac{\mathcal{N}\left(\boldsymbol{x}_t ; \sqrt{\alpha_t} \boldsymbol{x}_{t-1},\left(1-\alpha_t\right) \mathbf{I}\right) \mathcal{N}\left(\boldsymbol{x}_{t-1} ; \sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0,\left(1-\bar{\alpha}_{t-1}\right) \mathbf{I}\right)}{\mathcal{N}\left(\boldsymbol{x}_t ; \sqrt{\bar{\alpha}_t} \boldsymbol{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)} \\
        & \propto \exp \left\{-\left[\frac{\left(\boldsymbol{x}_t-\sqrt{\alpha_t} \boldsymbol{x}_{t-1}\right)^2}{2\left(1-\alpha_t\right)}+\frac{\left(\boldsymbol{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0\right)^2}{2\left(1-\bar{\alpha}_{t-1}\right)}-\frac{\left(\boldsymbol{x}_t-\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0\right)^2}{2\left(1-\bar{\alpha}_t\right)}\right]\right\} \\
        & =\exp \left\{-\frac{1}{2}\left[\frac{\left(\boldsymbol{x}_t-\sqrt{\alpha_t} \boldsymbol{x}_{t-1}\right)^2}{1-\alpha_t}+\frac{\left(\boldsymbol{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0\right)^2}{1-\bar{\alpha}_{t-1}}-\frac{\left(\boldsymbol{x}_t-\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0\right)^2}{1-\bar{\alpha}_t}\right]\right\} \\
        & =\exp \left\{-\frac{1}{2}\left[\frac{\left(-2 \sqrt{\alpha_t} \boldsymbol{x}_t \boldsymbol{x}_{t-1}+\alpha_t \boldsymbol{x}_{t-1}^2\right)}{1-\alpha_t}+\frac{\left(\boldsymbol{x}_{t-1}^2-2 \sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_{t-1} \boldsymbol{x}_0\right)}{1-\bar{\alpha}_{t-1}}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
        & where \ C(x_t, x_0) = \frac{x_{t}^2}{1 - \alpha_t} + \frac{\bar{\alpha}_{t-1}x_{0}^2}{1 - \bar{\alpha}_{t-1}} - \frac{(x_t - \sqrt{\bar{\alpha}_{t}}x_0)^2}{1 - \bar{\alpha}_t} \\
        & \propto \exp \left\{-\frac{1}{2}\left[-\frac{2 \sqrt{\alpha_t} \boldsymbol{x}_t \boldsymbol{x}_{t-1}}{1-\alpha_t}+\frac{\alpha_t \boldsymbol{x}_{t-1}^2}{1-\alpha_t}+\frac{\boldsymbol{x}_{t-1}^2}{1-\bar{\alpha}_{t-1}}-\frac{2 \sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_{t-1} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
        & =\exp \left\{-\frac{1}{2}\left[\left(\frac{\alpha_t}{1-\alpha_t}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
        & =\exp \left\{-\frac{1}{2}\left[\frac{\alpha_t\left(1-\bar{\alpha}_{t-1}\right)+1-\alpha_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)} \boldsymbol{x}_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
        & =\exp \left\{-\frac{1}{2}\left[\frac{\alpha_t-\bar{\alpha}_t+1-\alpha_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)} x_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} x_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} x_0}{1-\bar{\alpha}_{t-1}}\right) x_{t-1}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
        & =\exp \left\{-\frac{1}{2}\left[\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)} x_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} x_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} x_0}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
        & =\exp \left\{-\frac{1}{2}\left(\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}\right)\left[\boldsymbol{x}_{t-1}^2-2 \frac{\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right)}{\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}} \boldsymbol{x}_{t-1}+\widetilde{C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)}\right]\right\} \\
        & where \ \widetilde{C(x_t, x_0)} = \frac{C(x_t, x_0)(1 - \alpha_t)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \\
        & =\exp \left\{-\frac{1}{2}\left(\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}\right)\left[\boldsymbol{x}_{t-1}^2-2 \frac{\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right)\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \boldsymbol{x}_{t-1}+\widetilde{C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)}\right]\right\} \\
        & =\exp \left\{-\frac{1}{2}\left(\frac{1}{\frac{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t}}\right)\left[x_{t-1}^2-2 \frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) x_t+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_t\right) x_0}{1-\bar{\alpha}_t} x_{t-1}+\widetilde{C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)}\right]\right\} \\
        & =\exp \left\{-\frac{1}{2}\left(\frac{1}{\frac{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t}}\right)\left[x_{t-1} - \frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) x_t+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_t\right) x_0}{1-\bar{\alpha}_t}\right]^2\right\} \\
        & \propto \mathcal{N}(\boldsymbol{x}_{t-1} ; \underbrace{\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) \boldsymbol{x}_t+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_t\right) \boldsymbol{x}_0}{1-\bar{\alpha}_t}}_{\mu_q\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)}, \underbrace{\left.\frac{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \mathbf{I}\right)}_{\boldsymbol{\Sigma}_q(t)} \\
        \end{aligned}\]</span></p>
        <p>Since we want to minimize the DKL, we assume ptheta has a similar
        form as the analytical form found for q(xt-1|xt, x0). Therefore we fix
        ptheta = ...</p>
        <p>Given that, we can finally have an explicit form for our sum to
        minimize from the previous calculus of the ELBO :</p>
        <p>Calculus of the ELBO next part</p>
        <p>Therefore, optimizing a VDM boils down to learning a neural network
        to predict the original ground truth image from an arbitrarily noisified
        version of it [5]. Furthermore, minimizing the summation term of our
        derived ELBO objective (Equation 58) across all noise levels can be
        approximated by minimizing the expectation over all time-step, which
        leads to the final objective function :</p>
        <p>This function can be optimized using MCMC method to calculate
        numerically the expectations and adjust the parameters theta of our
        neural network in order to minimize the loss function above, by gradient
        descent for example.</p>
        <h1 id="improvement">Improvement</h1>
        <p>Interperation with noise</p>
        <p>Conclusion</p>
    </div>

</body>
</html>
