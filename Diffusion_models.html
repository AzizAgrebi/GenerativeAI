<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Mathematics behind diffusion models (Article in construction) Aziz Agrebi</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Mathematics behind diffusion models (Article in
construction)<br />
Aziz Agrebi</h1>
</header>
<h1 id="introduction">Introduction</h1>
<p>Diffusion models have emerged as a leading class of generative
models, capable of generating diverse and high-resolution images. Their
prominence escalated notably when major entities, such as OpenAI and
Google, achieved success in training highly performing models like
DALLE-2 and Imagen. In this article, we will delve into the mathematical
foundations of diffusion models.</p>
<h1 id="the-key-idea">The key idea</h1>
<p>When confronted with observed samples x stemming from a distribution
of interest — considering these samples as images — the aim of a
generative model is to effectively capture and model the underlying data
distribution <span class="math inline">\(q(x)\)</span>. Once this
modeling is achieved, the model gains the capability to generate new
samples at will. In the case of diffusion models, the approach to
estimating the true data distribution <span
class="math inline">\(q(x)\)</span> involves the following methodical
process : It begins by taking an initial image <span
class="math inline">\(x_0\)</span> and gradually adding Gaussian noise
to it through a series of T steps. The goal is that after T steps, the
noised image <span class="math inline">\(x_T\)</span> follows a normal
distribution. This step is called the forward diffusion process.
Following this, a neural network is trained to reverse the noising
process and recover the original data. This is the ability to
proficiently model the reverse process that enables the diffusion model
to generate new data. Indeed, if we learn to reverse the noising
process, we can sample a random noise from the normal distribution (the
distribution of <span class="math inline">\(xT\)</span> and apply the
reverse of the noising process to generate a new data that follows the
same distribution as x0. This is the called the reverse diffusion
process or, in general, the sampling process of the diffusion model. The
key is to successfully learn the reverse process of our noising
process.</p>
<h1 id="the-forward-diffusion-process">The forward diffusion
process</h1>
<p>Given a data-point <span class="math inline">\(x_0\)</span> sampled
from the real data distribution <span
class="math inline">\(q(x)\)</span> (<span class="math inline">\(x_0
\sim q(x)\)</span>), we establish the forward diffusion process by
incorporating Gaussian noise at each step. More precisely, at every step
<span class="math inline">\(t\)</span> (<span class="math inline">\(1
\leq t \leq T\)</span>) of this process, we introduce Gaussian noise
with a variance of <span class="math inline">\(\beta_t\)</span> to <span
class="math inline">\(x_{t-1}\)</span>, producing a new latent variable
<span class="math inline">\(x_t\)</span> with distribution <span
class="math inline">\(q(x_t | x_{t-1})\)</span>. This diffusion process
is formulated as follows:</p>
<p><span class="math display">\[\forall t \quad q(x_t | x_{t-1}) =
\mathcal{N}(x_t; \mu_t = (1 - \beta_t) x_{t-1}, \Sigma_t = \beta_t
I)\]</span></p>
<p>Each latent variable <span class="math inline">\(x_t\)</span> only
depends on the previous variable <span
class="math inline">\(x_{t-1}\)</span>, making our diffusion process a
Markov chain.</p>
<p>As mentioned previously, we desire <span
class="math inline">\(x_T\)</span> to follow the normal distribution,
meaning <span class="math inline">\(x_T \sim \mathcal{N}(x_T; \mu_T = 0,
\Sigma_T = I)\)</span>. We will revisit this when discussing our
objective function.</p>
<p>We can conclude this part by calculating the distribution of the
trajectory of our forward diffusion process conditioned on the given
data-point <span class="math inline">\(x_0\)</span>. The trajectory is a
sequence of states that the system traverses as it evolves over time
(excluding the first state here). Mathematically, the trajectory of our
process is represented as the sequence <span class="math inline">\((x_1,
\ldots, x_{T-1}, x_T)\)</span> and is denoted <span
class="math inline">\(x_{1:T}\)</span>. Therefore, we can write:</p>
<p><span class="math display">\[\begin{aligned}
    q(x_{1:T} | x_0) &amp;= \frac{q(x_{1:T}, x_0)}{q(x_0)} \quad
(\text{Bayes&#39; rule}) \\
    &amp;= \frac{q(x_0)\prod_{t=1}^{T} q(x_t | x_{t-1})}{q(x_0)} \quad
(\text{Markov property}) \\
    &amp;= \prod_{t=1}^{T} q(x_t | x_{t-1})
\end{aligned}\]</span></p>
<h1 id="the-reverse-diffusion-process">The reverse diffusion
process</h1>
<p>The reverse diffusion process is the process that enables to start
from <span class="math inline">\(x_T\)</span> (which we assume for now
to follow a normal distribution) and to finish at a new <span
class="math inline">\(x_0\)</span> that we will denote <span
class="math inline">\(\tilde{x_0}\)</span>, which means a new sample
that follows the distribution <span class="math inline">\(q(x_0) :=
q(x)\)</span>. Therefore we fix <span class="math inline">\(p(x_T) =
\mathcal{N}(x_T; \mu_T = 0, \Sigma_T = I)\)</span> and we want to learn
<span class="math inline">\(q(x_{t-1} | x_t)\)</span> for every t. In
the general case, each <span class="math inline">\(q(x_{t-1} |
x_t)\)</span> has no explicit analytical form so we have to train a
model to approximate this distribution. We will denote <span
class="math inline">\(p_{\theta}(x_{t-1} | x_t)\)</span> the
approximation learnt by our model. The model we will use is a U-Net
model, its parameters are denoted theta here, and we will train this
model by maximizing the log likelihood. We will detail all of this in
the next part. One remark is that it was proven that, if the noise added
at each step during the forward diffusion process is small enough (so if
<span class="math inline">\(\beta_t\)</span> is small enough), each
<span class="math inline">\(q(x_{t-1} | x_t)\)</span> is a Gaussian
distribution. Therefore in this approximation we can write <span
class="math inline">\(q(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_t,
\Sigma_t)\)</span> and train our model to only learn the mean and the
variance of this family of distributions by maximizing the log
likelihood. In this article, we choose another method. We will show that
even if <span class="math inline">\(q(x_{t-1} | x_t)\)</span> has no
explicit analytical form, we can find an explicit analytical form for
<span class="math inline">\(q(x_{t-1} | x_t, x_0)\)</span> and we will
try to learn a similar form for <span class="math inline">\(q(x_{t-1} |
x_t)\)</span> with our model. Everything will make sense when we will
define our objective function to train our model. Finally, we can do
like in the previous part and write the distribution of the trajectory
or our learn reverse diffusion process :</p>
<h1 id="training-a-diffusion-model">Training a diffusion model</h1>
<p>In order to learn <span class="math inline">\(p_{\theta}(x_{t-1} |
x_t)\)</span> with our model to match closely <span
class="math inline">\(q(x_{t-1} | x_t)\)</span>, we will maximize the
log-likelihood <span class="math inline">\(ln(q(x))\)</span>. To do so,
let’s introduce the Evidence Lower Bound (ELBO). For a given
distribution phi, the general definition of the ELBO is :</p>
<p>It is possible to show a very interesting relation between the log
likelihood and the ELBO :</p>
<p>Interpretation (Log likeli hood = ELBO + DKL)</p>
<p>This is why for now, we don’t explicitly want to maximize the log
likelihood but rather the ELBO (which is equivalent given our previous
result). With a calculus, we can exhibit a very interesting form for the
ELBO in the case of our diffusion model :</p>
<p><span class="math display">\[\begin{aligned}
    \ln p(x) &amp;= \ln \int p(x_{0:T}) \, dx_{1:T}\\
    &amp;= \ln \int \frac{p(x_{0:T}) q(x_{1:T}|x_{0})}{q(x_{1:T}|x_{0})}
\, dx_{1:T} \\
    &amp;= \ln\left(\mathbb{E}_{q(x_{1:T}|x_{0})} \left[
\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})} \right]\right) \\
    &amp;\geq \mathbb{E}_{q(x_{1:T}|x_0)}
\left[\ln\left(\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}\right)\right] \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
\prod_{t=1}^{T} p_{\theta}(x_{t-1}|x_t)}{\prod_{t=1}^{T}
q(x_t|x_{t-1})}\right)\right] \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
p_{\theta}(x_0|x_1) \prod_{t=2}^{T} p_{\theta}(x_{t-1}|x_t)}{q(x_1|x_0)
\prod_{t=2}^{T} q(x_t|x_{t-1})}\right)\right] \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
p_{\theta}(x_0|x_1)}{q(x_1|x_0)}\right) + \ln\left( \prod_{t=2}^{T}
\frac{p_{\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1})}\right) \right]\\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
p_{\theta}(x_0|x_1)}{q(x_1|x_0)}\right) + \ln\left( \prod_{t=2}^{T}
\frac{p_{\theta}(x_{t-1}|x_t)}{q(x_t|x_{t-1}, x_0)}\right) \right]\\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
p_{\theta}(x_0|x_1)}{q(x_1|x_0)}\right) + \ln\left( \prod_{t=2}^{T}
\frac{p_{\theta}(x_{t-1}|x_t)}{\frac{q(x_{t-1}|x_t,
x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}}\right) \right] \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
p_{\theta}(x_0|x_1)}{q(x_1|x_0)}\right) +
\ln\left(\frac{q(x_1|x_0))}{q(x_T|x_0)}\right) + \ln\left(
\prod_{t=2}^{T} \frac{p_{\theta}(x_{t-1}|x_t)}{q(x_{t-1}|x_t,
x_0)}\right) \right] \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln\left(\frac{p(x_T)
p_{\theta}(x_0|x_1)}{q(x_T|x_0)}\right) + \sum_{t=2}^{T}
\ln\left(  \frac{p_{\theta}(x_{t-1}|x_t)}{q(x_{t-1}|x_t, x_0)}\right)
\right] \\
    &amp;= \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln
p_{\theta}(x_0|x_1)\right] + \mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln
\frac{p(x_T)}{q(x_T|x_0)} \right] + \sum_{t=2}^{T}
\mathbb{E}_{q(x_{1:T}|x_0)} \left[\ln
\frac{p_{\theta}(x_{t-1}|x_t)}{q(x_{t-1}|x_t, x_0)} \right] \\
    &amp;= \mathbb{E}_{q(x_1|x_0)} \left[\ln p_{\theta}(x_0|x_1) \right]
+ \mathbb{E}_{q(x_T|x_0)} \left[\ln \frac{p(x_T)}{q(x_T|x_0)} \right] +
\sum_{t=2}^{T} \mathbb{E}_{q(x_t, x_{t-1}|x_0)}
\left[\ln   \frac{p_{\theta}(x_{t-1}|x_t)}{q(x_{t-1}|x_t, x_0)} \right]
\\
    &amp;= \underbrace{{\mathbb{E}_{q(x_1|x_0)} \left[\ln
p_{\theta}(x_0|x_1) \right]}}_{\text{Reconstruction term}} -
\underbrace{D_{KL}\left(q(x_T | x_0) \| p(x_T)\right)}_{\text{Prior
matching term}} - \underbrace{\sum_{t=2}^{T} \mathbb{E}_{q(x_t|x_0)}
\left[D_{KL}\left(q(x_{t-1} | x_t, x_0) \|
p_{\theta}(x_{t-1}|x_t)\right)\right]}_{\text{Denoising matching term}}
\end{aligned}\]</span></p>
<p>Calculus of the ELBO.</p>
<p>Interpretation of the 3 terms.</p>
<p>Therefore, maximizing the ELBO is pratically equivalent to minimizing
the third term of our previous written ELBO. ptheta(xt-1|xt) is what we
want to learn in order to minimize this sum. To be able to do such a
thing in practice, we need to make sure that we can calculate the other
terms of the sum, which means q(xt|x0) and q(xt-1|xt, x0) for every t.
Fortunately, this is the case.</p>
<p>(1) calculus of <span class="math inline">\(q(x_t|x0)\)</span></p>
<p><span class="math display">\[\begin{aligned}
    \boldsymbol{x}_t &=\sqrt{\alpha_t} \boldsymbol{x}_{t-1}+\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1} \\
    &= \sqrt{\alpha_t}\left(\sqrt{\alpha_{t-1}} \boldsymbol{x}_{t-2}+\sqrt{1-\alpha_{t-1}} \boldsymbol{\epsilon}_{t-2}\right)+\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1} \\
    & =\sqrt{\alpha_t \alpha_{t-1}} \boldsymbol{x}_{t-2}+\sqrt{\alpha_t-\alpha_t \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2}+\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1} \\
\\
&\text But : \\
\\
    & \sqrt{\alpha_t-\alpha_t \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2}+\sqrt{1-\alpha_t} \boldsymbol{\epsilon}_{t-1} \\
    & \sim \sqrt{\alpha_t-\alpha_t \alpha_{t-1}} \mathcal{N}\left(0, \mathbf{I}\right) + \sqrt{1-\alpha_t} \mathcal{N}\left(0, \mathbf{I}\right) \\
    & \sim \mathcal{N}\left(0, (\alpha_t-\alpha_t \alpha_{t-1}) \mathbf{I}\right) + \mathcal{N}\left(0, (1-\alpha_t) \mathbf{I}\right) \\
    & \sim \mathcal{N}\left(0, (\alpha_t-\alpha_t \alpha_{t-1} + 1-\alpha_t) \mathbf{I}\right)  \\
    & \sim \mathcal{N}\left(0, (1-\alpha_t \alpha_{t-1}) \mathbf{I}\right)  \\
    & \sim \sqrt{1-\alpha_t \alpha_{t-1}} \mathcal{N}\left(0, \mathbf{I}\right)  \\
\\
&\text Therefore : \\
\\
    & \boldsymbol{x}_t =\sqrt{\alpha_t \alpha_{t-1}} \boldsymbol{x}_{t-2}+\sqrt{1-\alpha_t \alpha_{t-1}} \boldsymbol{\epsilon}_{t-2} \\
    & =\ldots \\
    & =\sqrt{\prod_{i=1}^t \alpha_i} \boldsymbol{x}_0+\sqrt{1-\prod_{i=1}^t \alpha_i} \boldsymbol{\epsilon}_0 \\
    & =\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_0 \\
    & \sim \mathcal{N}\left(\boldsymbol{x}_t ; \sqrt{\bar{\alpha}_t} \boldsymbol{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)
\end{aligned}\]</span></p>
</div>
<p>Where <span class="math inline">\(\bar{\alpha}_t := \prod_{i=1}^t
\alpha_i\)</span></p>
<p>(2) calculus of q(xt-1|xt, x0)</p>
<p><span class="math display">\[\begin{aligned}
& q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_t, \boldsymbol{x}_0\right)=\frac{q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_{t-1}, \boldsymbol{x}_0\right) q\left(\boldsymbol{x}_{t-1} \mid \boldsymbol{x}_0\right)}{q\left(\boldsymbol{x}_t \mid \boldsymbol{x}_0\right)} \\
& =\frac{\mathcal{N}\left(\boldsymbol{x}_t ; \sqrt{\alpha_t} \boldsymbol{x}_{t-1},\left(1-\alpha_t\right) \mathbf{I}\right) \mathcal{N}\left(\boldsymbol{x}_{t-1} ; \sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0,\left(1-\bar{\alpha}_{t-1}\right) \mathbf{I}\right)}{\mathcal{N}\left(\boldsymbol{x}_t ; \sqrt{\bar{\alpha}_t} \boldsymbol{x}_0,\left(1-\bar{\alpha}_t\right) \mathbf{I}\right)} \\
& \propto \exp \left\{-\left[\frac{\left(\boldsymbol{x}_t-\sqrt{\alpha_t} \boldsymbol{x}_{t-1}\right)^2}{2\left(1-\alpha_t\right)}+\frac{\left(\boldsymbol{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0\right)^2}{2\left(1-\bar{\alpha}_{t-1}\right)}-\frac{\left(\boldsymbol{x}_t-\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0\right)^2}{2\left(1-\bar{\alpha}_t\right)}\right]\right\} \\
& =\exp \left\{-\frac{1}{2}\left[\frac{\left(\boldsymbol{x}_t-\sqrt{\alpha_t} \boldsymbol{x}_{t-1}\right)^2}{1-\alpha_t}+\frac{\left(\boldsymbol{x}_{t-1}-\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0\right)^2}{1-\bar{\alpha}_{t-1}}-\frac{\left(\boldsymbol{x}_t-\sqrt{\bar{\alpha}_t} \boldsymbol{x}_0\right)^2}{1-\bar{\alpha}_t}\right]\right\} \\
& =\exp \left\{-\frac{1}{2}\left[\frac{\left(-2 \sqrt{\alpha_t} \boldsymbol{x}_t \boldsymbol{x}_{t-1}+\alpha_t \boldsymbol{x}_{t-1}^2\right)}{1-\alpha_t}+\frac{\left(\boldsymbol{x}_{t-1}^2-2 \sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_{t-1} \boldsymbol{x}_0\right)}{1-\bar{\alpha}_{t-1}}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
& where \ C(x_t, x_0) = \frac{x_{t}^2}{1 - \alpha_t} + \frac{\bar{\alpha}_{t-1}x_{0}^2}{1 - \bar{\alpha}_{t-1}} - \frac{(x_t - \sqrt{\bar{\alpha}_{t}}x_0)^2}{1 - \bar{\alpha}_t} \\
& \propto \exp \left\{-\frac{1}{2}\left[-\frac{2 \sqrt{\alpha_t} \boldsymbol{x}_t \boldsymbol{x}_{t-1}}{1-\alpha_t}+\frac{\alpha_t \boldsymbol{x}_{t-1}^2}{1-\alpha_t}+\frac{\boldsymbol{x}_{t-1}^2}{1-\bar{\alpha}_{t-1}}-\frac{2 \sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_{t-1} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
& =\exp \left\{-\frac{1}{2}\left[\left(\frac{\alpha_t}{1-\alpha_t}+\frac{1}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
& =\exp \left\{-\frac{1}{2}\left[\frac{\alpha_t\left(1-\bar{\alpha}_{t-1}\right)+1-\alpha_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)} \boldsymbol{x}_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
& =\exp \left\{-\frac{1}{2}\left[\frac{\alpha_t-\bar{\alpha}_t+1-\alpha_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)} x_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} x_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} x_0}{1-\bar{\alpha}_{t-1}}\right) x_{t-1}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
& =\exp \left\{-\frac{1}{2}\left[\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)} x_{t-1}^2-2\left(\frac{\sqrt{\alpha_t} x_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} x_0}{1-\bar{\alpha}_{t-1}}\right) \boldsymbol{x}_{t-1}+C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)\right]\right\} \\
& =\exp \left\{-\frac{1}{2}\left(\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}\right)\left[\boldsymbol{x}_{t-1}^2-2 \frac{\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right)}{\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}} \boldsymbol{x}_{t-1}+\widetilde{C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)}\right]\right\} \\
& where \ \widetilde{C(x_t, x_0)} = \frac{C(x_t, x_0)(1 - \alpha_t)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \\
& =\exp \left\{-\frac{1}{2}\left(\frac{1-\bar{\alpha}_t}{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}\right)\left[\boldsymbol{x}_{t-1}^2-2 \frac{\left(\frac{\sqrt{\alpha_t} \boldsymbol{x}_t}{1-\alpha_t}+\frac{\sqrt{\bar{\alpha}_{t-1}} \boldsymbol{x}_0}{1-\bar{\alpha}_{t-1}}\right)\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \boldsymbol{x}_{t-1}+\widetilde{C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)}\right]\right\} \\
& =\exp \left\{-\frac{1}{2}\left(\frac{1}{\frac{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t}}\right)\left[x_{t-1}^2-2 \frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) x_t+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_t\right) x_0}{1-\bar{\alpha}_t} x_{t-1}+\widetilde{C\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)}\right]\right\} \\
& =\exp \left\{-\frac{1}{2}\left(\frac{1}{\frac{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t}}\right)\left[x_{t-1} - \frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) x_t+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_t\right) x_0}{1-\bar{\alpha}_t}\right]^2\right\} \\
& \propto \mathcal{N}(\boldsymbol{x}_{t-1} ; \underbrace{\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right) \boldsymbol{x}_t+\sqrt{\bar{\alpha}_{t-1}}\left(1-\alpha_t\right) \boldsymbol{x}_0}{1-\bar{\alpha}_t}}_{\mu_q\left(\boldsymbol{x}_t, \boldsymbol{x}_0\right)}, \underbrace{\left.\frac{\left(1-\alpha_t\right)\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} \mathbf{I}\right)}_{\boldsymbol{\Sigma}_q(t)} \\
\end{aligned}\]</span></p>
</div>
<p>Since we want to minimize the DKL, we assume ptheta has a similar
form as the analytical form found for q(xt-1|xt, x0). Therefore we fix
ptheta = ...</p>
<p>Given that, we can finally have an explicit form for our sum to
minimize from the previous calculus of the ELBO :</p>
<p>Calculus of the ELBO next part</p>
<p>Therefore, optimizing a VDM boils down to learning a neural network
to predict the original ground truth image from an arbitrarily noisified
version of it [5]. Furthermore, minimizing the summation term of our
derived ELBO objective (Equation 58) across all noise levels can be
approximated by minimizing the expectation over all time-step, which
leads to the final objective function :</p>
<p>This function can be optimized using MCMC method to calculate
numerically the expectations and adjust the parameters theta of our
neural network in order to minimize the loss function above, by gradient
descent for example.</p>
<h1 id="improvement">Improvement</h1>
<p>Interperation with noise</p>
<p>Conclusion</p>
</body>
</html>
